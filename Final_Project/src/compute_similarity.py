"""
Script tÃ­nh item-item similarity matrix (ULTRA OPTIMIZED)
Chá»‰ lÆ°u Top-N similar items thay vÃ¬ toÃ n bá»™ matrix
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix
import pickle
from pathlib import Path
from tqdm import tqdm

def compute_item_similarity_topn(min_ratings=10, top_n=50):
    """
    TÃ­nh cosine similarity nhÆ°ng CHá»ˆ LÆ¯U TOP-N items tÆ°Æ¡ng tá»± nháº¥t
    Tiáº¿t kiá»‡m memory cá»±c ká»³ nhiá»u

    Args:
        min_ratings: Chá»‰ giá»¯ books cÃ³ Ã­t nháº¥t N ratings
        top_n: Chá»‰ lÆ°u N items tÆ°Æ¡ng tá»± nháº¥t cho má»—i item
    """
    print("ğŸ“š Äang load dá»¯ liá»‡u...")

    # Load ratings data
    ratings_path = Path('../dataset/cleaned/Ratings_cleaned.csv')
    ratings = pd.read_csv(ratings_path)

    print(f"âœ… Loaded {len(ratings):,} ratings")
    print(f"ğŸ“– Sá»‘ books: {ratings['ISBN'].nunique():,}")
    print(f"ğŸ‘¥ Sá»‘ users: {ratings['User-ID'].nunique():,}")

    # Filter: Chá»‰ giá»¯ books cÃ³ Ä‘á»§ ratings
    print(f"\nğŸ”§ Filtering books vá»›i Ã­t nháº¥t {min_ratings} ratings...")
    book_counts = ratings['ISBN'].value_counts()
    popular_books = book_counts[book_counts >= min_ratings].index.tolist()
    ratings_filtered = ratings[ratings['ISBN'].isin(popular_books)]

    print(f"âœ… CÃ²n láº¡i {len(ratings_filtered):,} ratings")
    print(f"ğŸ“– CÃ²n láº¡i {ratings_filtered['ISBN'].nunique():,} books")

    # Táº¡o user-item matrix
    print("\nğŸ”§ Äang táº¡o user-item matrix...")
    user_item_matrix = ratings_filtered.pivot_table(
        index='User-ID',
        columns='ISBN',
        values='Book-Rating',
        fill_value=0
    )

    print(f"ğŸ“Š Matrix shape: {user_item_matrix.shape}")

    # Convert to sparse matrix
    print("\nğŸ’¾ Converting to sparse matrix...")
    sparse_matrix = csr_matrix(user_item_matrix.values)
    isbns = user_item_matrix.columns.tolist()
    n_items = len(isbns)

    print(f"âœ… Sparse matrix: {sparse_matrix.shape}")
    print(f"ğŸ’¾ Sparsity: {100 * (1 - sparse_matrix.nnz / (sparse_matrix.shape[0] * sparse_matrix.shape[1])):.2f}%")

    # TÃ­nh similarity tá»«ng item má»™t vÃ  chá»‰ lÆ°u top-N
    print(f"\nğŸ§® Äang tÃ­nh top-{top_n} similar items cho má»—i book...")
    print("â³ ÄÃ¢y sáº½ máº¥t vÃ i phÃºt, vui lÃ²ng Ä‘á»£i...")

    # Dictionary Ä‘á»ƒ lÆ°u top-N similarities
    # Format: {isbn: [(similar_isbn, score), ...]}
    similarity_dict = {}

    # TÃ­nh theo batch nhá» Ä‘á»ƒ trÃ¡nh memory overflow
    batch_size = 100

    for i in range(0, n_items, batch_size):
        end = min(i + batch_size, n_items)
        print(f"   Processing items {i+1} to {end} / {n_items}...")

        # TÃ­nh similarity cho batch nÃ y vá»›i Táº¤T Cáº¢ items
        batch_similarity = cosine_similarity(
            sparse_matrix[:, i:end].T,  # Batch hiá»‡n táº¡i
            sparse_matrix.T              # ToÃ n bá»™ items
        )

        # Vá»›i má»—i item trong batch, lÆ°u top-N similar items
        for j, isbn_idx in enumerate(range(i, end)):
            isbn = isbns[isbn_idx]
            scores = batch_similarity[j]

            # Láº¥y indices cá»§a top-N (bá» chÃ­nh nÃ³)
            # argsort tráº£ vá» indices, [::-1] Ä‘á»ƒ reverse (cao nháº¥t trÆ°á»›c)
            top_indices = np.argsort(scores)[::-1][1:top_n+1]  # Bá» index 0 (chÃ­nh nÃ³)

            # LÆ°u (isbn, score) pairs
            similar_items = [
                (isbns[idx], float(scores[idx]))
                for idx in top_indices
                if scores[idx] > 0.01  # Chá»‰ lÆ°u náº¿u similarity > threshold
            ]

            similarity_dict[isbn] = similar_items

    print(f"\nâœ… ÄÃ£ tÃ­nh similarity cho {len(similarity_dict):,} books")

    # LÆ°u file
    print("\nğŸ’¾ Äang lÆ°u file...")
    output_path = Path('../dataset/cleaned/item_similarity.pkl')
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'wb') as f:
        pickle.dump(similarity_dict, f)

    print(f"âœ… ÄÃ£ lÆ°u similarity dictionary táº¡i: {output_path}")
    print(f"ğŸ“¦ File size: {output_path.stat().st_size / (1024*1024):.2f} MB")

    # Test thá»­
    print("\nğŸ§ª Testing similarity dictionary...")
    sample_isbn = list(similarity_dict.keys())[0]
    similar_books = similarity_dict[sample_isbn]
    print(f"\nTop 5 books tÆ°Æ¡ng tá»± vá»›i ISBN {sample_isbn}:")
    for isbn, score in similar_books[:5]:
        print(f"  {isbn}: {score:.4f}")

    # Statistics
    avg_similar = np.mean([len(v) for v in similarity_dict.values()])
    print(f"\nğŸ“Š Thá»‘ng kÃª:")
    print(f"   Trung bÃ¬nh má»—i book cÃ³ {avg_similar:.1f} similar items")

    return similarity_dict

if __name__ == "__main__":
    print("="*60)
    print("ğŸš€ TÃNH ITEM-ITEM SIMILARITY (ULTRA OPTIMIZED)")
    print("="*60)
    print("\nğŸ’¡ PhiÃªn báº£n nÃ y CHá»ˆ LÆ¯U TOP-N similar items")
    print("   thay vÃ¬ toÃ n bá»™ matrix â†’ tiáº¿t kiá»‡m memory cá»±c nhiá»u!")

    # Há»i user vá» parameters
    print("\nâš™ï¸  Cáº¥u hÃ¬nh:")
    print("   - min_ratings: Chá»‰ giá»¯ books cÃ³ Ã­t nháº¥t N ratings")
    print("   - top_n: Sá»‘ lÆ°á»£ng similar items lÆ°u cho má»—i book")

    min_ratings_input = input("\nNháº­p min_ratings [máº·c Ä‘á»‹nh: 10]: ").strip()
    min_ratings = int(min_ratings_input) if min_ratings_input else 10

    top_n_input = input("Nháº­p top_n [máº·c Ä‘á»‹nh: 50]: ").strip()
    top_n = int(top_n_input) if top_n_input else 50

    print(f"\nğŸ“Œ Cáº¥u hÃ¬nh: min_ratings={min_ratings}, top_n={top_n}")

    try:
        similarity_dict = compute_item_similarity_topn(
            min_ratings=min_ratings,
            top_n=top_n
        )

        print("\n" + "="*60)
        print("âœ… HOÃ€N THÃ€NH!")
        print("="*60)
        print("\nğŸ“Œ File Ä‘Ã£ táº¡o: ../dataset/cleaned/item_similarity.pkl")
        print("ğŸ“Œ BÃ¢y giá» báº¡n cÃ³ thá»ƒ cháº¡y: streamlit run app.py")
        print("\nâš ï¸  LÆ¯U Ã: File similarity giá» lÃ  DICTIONARY, khÃ´ng pháº£i DataFrame")
        print("   NhÆ°ng code trong app.py vÃ  utils.py váº«n hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng!")

    except MemoryError:
        print("\n" + "="*60)
        print("âŒ VáºªN Háº¾T MEMORY!")
        print("="*60)
        print("\nğŸ’¡ Giáº£i phÃ¡p:")
        print("1. TÄƒng min_ratings lÃªn 20 hoáº·c 30")
        print("2. Giáº£m top_n xuá»‘ng 20 hoáº·c 30")
        print("3. Cháº¡y trÃªn mÃ¡y RAM cao hÆ¡n hoáº·c Google Colab")

    except Exception as e:
        print(f"\nâŒ Lá»–I: {e}")
        import traceback
        traceback.print_exc()